{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81ddffa",
   "metadata": {},
   "source": [
    "# Entrenamiento modelo para detección de Phising a partir de una URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f781114c",
   "metadata": {},
   "source": [
    "Importamos la librerías necesarias para la realización del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Detectar automáticamente Java 17 o 11 (PySpark 4.1.0+ requiere Java 17+)\n",
    "if 'JAVA_HOME' not in os.environ:\n",
    "    try:\n",
    "        # Intentar Java 17 primero (requerido para PySpark 4.1.0+)\n",
    "        java_home_17 = os.popen('/usr/libexec/java_home -v 17 2>/dev/null').read().strip()\n",
    "        if java_home_17:\n",
    "            os.environ['JAVA_HOME'] = java_home_17\n",
    "        else:\n",
    "            # Si no hay Java 17, intentar Java 11\n",
    "            java_home_11 = os.popen('/usr/libexec/java_home -v 11 2>/dev/null').read().strip()\n",
    "            if java_home_11:\n",
    "                os.environ['JAVA_HOME'] = java_home_11\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Path dinámico para warehouse (compatible con cualquier OS)\n",
    "warehouse_dir = os.path.join(tempfile.gettempdir(), \"spark-warehouse\")\n",
    "os.makedirs(warehouse_dir, exist_ok=True)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Ejemplo pySparkSQL\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", f\"file://{warehouse_dir}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60fd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from pyspark.sql import Row, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import NumericType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.param import *\n",
    "from pyspark.ml.tuning import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f6488",
   "metadata": {},
   "source": [
    "Guardamos el path del csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(os.getcwd(), \"phishing_features.csv\")\n",
    "print(f\"CSV path: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff8c43b",
   "metadata": {},
   "source": [
    "Cargamos el csv en un dataframe e imprimimos el schema del mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(csv_path, inferSchema=True, header=True)\n",
    "print(\"Elementos en DataFrame a partir de datos/personas.csv: \" + str(df.count()) + \"\\nEsquema: \")\n",
    "print (df.printSchema())\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be632345",
   "metadata": {},
   "source": [
    "Lo primero que vamos a comprobar es si nuestro dataset está desbalanceado o no. Al tener un problema binario, lo ideal sería tener un 50% de ejemplos de cada clase. En el caso de que la diferencia sea muy obvia, tendremos que aplicar ciertas medidas para que no afecte al rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_1 = df.filter(df['label'] == 1).count()\n",
    "count_0 = df.filter(df['label'] == 0).count()\n",
    "total = df.count()\n",
    "\n",
    "\n",
    "perc_0 = (count_0 / total) * 100\n",
    "perc_1 = (count_1 / total) * 100\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Benign (0): {count_0} records ({perc_0:.2f}%)\")\n",
    "print(f\"Phishing (1): {count_1} records ({perc_1:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed418e2d",
   "metadata": {},
   "source": [
    "Comprobamos que en efecto, nuestro dataset está desbalanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Benign (0)\", \"Phishing (1)\"]\n",
    "percentages = [perc_0, perc_1]\n",
    "\n",
    "plt.figure()\n",
    "bars = plt.bar(labels, percentages)\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.title(\"Label distribution (%)\")\n",
    "\n",
    "# Añadir el texto del porcentaje encima de cada barra\n",
    "for bar, perc in zip(bars, percentages):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height(),\n",
    "        f\"{perc:.2f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\"\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b70b5",
   "metadata": {},
   "source": [
    "Vamos a dejar pasar esto por ahora a ver que tal funciona una regresión logística con estos datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623249f",
   "metadata": {},
   "source": [
    "A continuación, vamos a buscar si nuestro dataset posee valores nulos. Muchos modelos no aceptan valores nulos en su entrenamiento. Si no los tratamos, probablemente nos salte un error y no podremos continuar hasta que lo solucionemos. Si, por lo que sea, el modelo acepta los valores nulos, estos pueden suponer un problema que afecta al rendimiento del modelo ya que perderemos información que puede ser valiosa en el entrenamiento o podemos caer en sesgos. Buscamos valores nulos en el dataset para tratarlo de forma adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = reduce(\n",
    "    lambda a, b: a | b,\n",
    "    [col(c).isNull() for c in df.columns]\n",
    ")\n",
    "\n",
    "n = df.filter(condition).count()\n",
    "df.filter(condition).show(n, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18461fd",
   "metadata": {},
   "source": [
    "Vemos que hay varios registros cuyo tld (dominio) es nulo. Vamos a recorrer todas las url cuyo tld sea nulo y vamos a asignarles su "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71705ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"tld\",\n",
    "    when(col(\"tld\").isNull(),    \n",
    "        element_at(split(lit(col(\"url\")), r\"\\.\"), -1).alias(\"tld\")\n",
    "    ).otherwise(col(\"tld\"))\n",
    ")\n",
    "\n",
    "df = df.cache()\n",
    "df.count()\n",
    "\n",
    "df.filter((col(\"url\") == 'google.com') | (col('url') == 'wikipedia.org') | (col('url') == 'safeexample99.net')).show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6d196",
   "metadata": {},
   "source": [
    "Vemos en tres de URL antes filtradas que ya no tienen tld nulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cdbfd",
   "metadata": {},
   "source": [
    "### Atención: es importante darle un valor numérico a cada dominio (los modelos no aceptan strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65263ecf",
   "metadata": {},
   "source": [
    "Vemos cuantos domonios y de cada tipo hay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f915c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "unique_tlds = df.select(\"tld\").distinct().count()\n",
    "print(f\"Total de TLDs únicos en el dataset: {unique_tlds}\")\n",
    "\n",
    "print(\"\\nTLDs más frecuentes:\")\n",
    "tld_counts = df.groupBy(\"tld\").count().orderBy(col(\"count\").desc())\n",
    "tld_counts.show(20, truncate=False)\n",
    "\n",
    "print(\"\\nNOTA: La conversión de TLD a numérico se realizará dentro de la Pipeline\")\n",
    "print(\"para asegurar que el StringIndexer aprenda solo del conjunto de entrenamiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db919864",
   "metadata": {},
   "source": [
    "## Implementación de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df5287",
   "metadata": {},
   "source": [
    "Dividimos nuestros datos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69dc903",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 12418\n",
    "train, test_total = df.randomSplit([0.8, 0.2], seed=seed)\n",
    "\n",
    "train.cache()\n",
    "test_total.cache()\n",
    "\n",
    "train_count = train.count() \n",
    "test_count_total = test_total.count()\n",
    "\n",
    "print(f\"Train count: {train_count}\")\n",
    "print(f\"Test count: {test_count_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efa88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val, test = df.randomSplit([0.5, 0.5], seed=seed)\n",
    "\n",
    "val.cache()\n",
    "test.cache()\n",
    "\n",
    "val_count = val.count()\n",
    "test_count = test.count()\n",
    "\n",
    "print(f\"Validation count: {val_count}\")\n",
    "print(f\"Test count: {test_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de089a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld_indexer = StringIndexer(\n",
    "    inputCol=\"tld\",\n",
    "    outputCol=\"tld_indexed\", \n",
    "    handleInvalid=\"keep\"  \n",
    ")\n",
    "\n",
    "\n",
    "feature_cols = ['url_length', 'num_dots', 'has_https', 'has_ip', 'num_subdirs', \n",
    "                'num_params', 'suspicious_words', 'tld_indexed', 'special_char_count', \n",
    "                'digits_count', 'entropy']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "logistic_regression = LogisticRegression(\n",
    "    maxIter=20, \n",
    "    regParam=0.01, \n",
    "    featuresCol='features', \n",
    "    labelCol='label'\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tld_indexer, assembler, logistic_regression])\n",
    "\n",
    "print(\"Pipeline creada con los siguientes stages:\")\n",
    "print(\"1. StringIndexer (TLD -> numérico)\")\n",
    "print(\"2. VectorAssembler (features -> vector)\")\n",
    "print(\"3. LogisticRegression (modelo)\")\n",
    "print(\"\\nEntrenando pipeline...\")\n",
    "\n",
    "\n",
    "pipeline_model = pipeline.fit(train)\n",
    "\n",
    "\n",
    "tld_indexer_model = pipeline_model.stages[0]  \n",
    "tld_labels = tld_indexer_model.labels\n",
    "print(\"\\nMapeo de TLD -> Índice numérico (aprendido del conjunto de entrenamiento):\")\n",
    "print(\"=\" * 60)\n",
    "for idx, tld in enumerate(tld_labels[:10]):  # Mostrar los primeros 10\n",
    "    print(f\"{tld:25s} -> {idx}\")\n",
    "if len(tld_labels) > 10:\n",
    "    print(f\"... y {len(tld_labels) - 10} TLDs más\")\n",
    "print(f\"\\nTotal de TLDs únicos aprendidos: {len(tld_labels)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nAplicando pipeline a datos de validación...\")\n",
    "prediction = pipeline_model.transform(val)\n",
    "\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce370b9b",
   "metadata": {},
   "source": [
    "Vemos que nos da un accuracy muy bueno, sin embargo, este resultado es engañoso. Vamos a consultar la matriz de confusión para que está ocurriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e811a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrizConfusion(prediction):\n",
    "    rdd = prediction.select(\"prediction\", \"label\") \\\n",
    "        .rdd.map(lambda r: (float(r[\"prediction\"]), float(r[\"label\"])))\n",
    "\n",
    "    metrics = MulticlassMetrics(rdd)\n",
    "    cm = metrics.confusionMatrix().toArray()\n",
    "\n",
    "    labels = np.unique(prediction.select(\"label\").toPandas())\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(labels)), labels)\n",
    "    plt.yticks(range(len(labels)), labels)\n",
    "\n",
    "    plt.xlabel(\"Predicción\")\n",
    "    plt.ylabel(\"Etiqueta real\")\n",
    "    plt.title(\"Matriz de confusión\")\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, int(cm[i, j]),\n",
    "                    ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    TN = cm[0,0]\n",
    "    FP = cm[0,1]\n",
    "    FN = cm[1,0]\n",
    "    TP = cm[1,1]\n",
    "\n",
    "    TPR = TP / (TP + FN) if (TP + FN) else 0.0   # recall clase 1\n",
    "    TNR = TN / (TN + FP) if (TN + FP) else 0.0   # recall clase 0\n",
    "\n",
    "    recall_medio = (TPR + TNR) / 2\n",
    "    print(f\"Recall medio (TPR+TNR)/2: {recall_medio:.4f}\")\n",
    "\n",
    "matrizConfusion(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a389f70",
   "metadata": {},
   "source": [
    "Comprobamos que al ser un problema excesivamente desbalanceado, el accuracy no nos sirve para medir el rendimiento del modelo. En la matriz de confusión vemos que el modelo clasifica todos los registros como clase 1 (phising). Al ser la mayoría ejemplos de clase 1, el accuracy nos sale muy alto. Sin embargo, el modelo no está funcionando bien porque no es capaz de clasificar correctamente un ejemplo que no es phising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4263e9e",
   "metadata": {},
   "source": [
    "Vamos a tomar ciertas medidas para balancear un poco los ejemplos del problema para ver si el rendimiento del modelo aumenta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b628c",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff9b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: MEDIDAS PARA TRATAR DESBALANCEO DE CLASES\n",
    "phising_examples = train.filter(col(\"label\") == 1)\n",
    "non_phising_examples = train.filter(col(\"label\") == 0)\n",
    "\n",
    "print(f\"Train original → phising: {phising_examples.count()}, no phising: {non_phising_examples.count()}\")\n",
    "\n",
    "fraction = non_phising_examples.count() / phising_examples.count()\n",
    "\n",
    "majority_under = phising_examples.sample(\n",
    "    withReplacement=False,\n",
    "    fraction=fraction,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_under = non_phising_examples.unionByName(majority_under)\n",
    "\n",
    "print(\"Train balanceado:\")\n",
    "train_under.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld_indexer = StringIndexer(\n",
    "    inputCol=\"tld\",\n",
    "    outputCol=\"tld_indexed\", \n",
    "    handleInvalid=\"keep\"  \n",
    ")\n",
    "\n",
    "\n",
    "feature_cols = ['url_length', 'num_dots', 'has_https', 'has_ip', 'num_subdirs', \n",
    "                'num_params', 'suspicious_words', 'tld_indexed', 'special_char_count', \n",
    "                'digits_count', 'entropy']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "logistic_regression = LogisticRegression(\n",
    "    maxIter=20, \n",
    "    regParam=0.01, \n",
    "    featuresCol='features', \n",
    "    labelCol='label'\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tld_indexer, assembler, logistic_regression])\n",
    "\n",
    "print(\"Pipeline creada con los siguientes stages:\")\n",
    "print(\"1. StringIndexer (TLD -> numérico)\")\n",
    "print(\"2. VectorAssembler (features -> vector)\")\n",
    "print(\"3. LogisticRegression (modelo)\")\n",
    "print(\"\\nEntrenando pipeline...\")\n",
    "\n",
    "\n",
    "pipeline_model = pipeline.fit(train_under)\n",
    "\n",
    "\n",
    "tld_indexer_model = pipeline_model.stages[0]  \n",
    "tld_labels = tld_indexer_model.labels\n",
    "print(\"\\nMapeo de TLD -> Índice numérico (aprendido del conjunto de entrenamiento):\")\n",
    "print(\"=\" * 60)\n",
    "for idx, tld in enumerate(tld_labels[:10]):  # Mostrar los primeros 10\n",
    "    print(f\"{tld:25s} -> {idx}\")\n",
    "if len(tld_labels) > 10:\n",
    "    print(f\"... y {len(tld_labels) - 10} TLDs más\")\n",
    "print(f\"\\nTotal de TLDs únicos aprendidos: {len(tld_labels)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nAplicando pipeline a datos de validación...\")\n",
    "prediction = pipeline_model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a9fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizConfusion(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca00f32",
   "metadata": {},
   "source": [
    "Podemos observar el recall medio por clases mejora considerablemente (de 0.5 a 0.9980). Vemos que ahora hay 400 ejemplos de la clase no-phising que se están clasificando correctamente, es decir, ya no clasifica todo como clase phising. Todavía hay bastantes falsos negativos pero en general los resultados son bastante satisifactorios aplicando undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a632cd72",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86069d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "phising_examples = train.filter(col(\"label\") == 1)\n",
    "non_phising_examples = train.filter(col(\"label\") == 0)\n",
    "\n",
    "print(f\"Train original → phising: {phising_examples.count()}, no phising: {non_phising_examples.count()}\")\n",
    "\n",
    "fraction = phising_examples.count() / non_phising_examples.count()\n",
    "\n",
    "majority_under = non_phising_examples.sample(\n",
    "    withReplacement=True,\n",
    "    fraction=fraction,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "train_over = phising_examples.unionByName(majority_under)\n",
    "\n",
    "print(\"Train balanceado:\")\n",
    "train_over.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tld_indexer = StringIndexer(\n",
    "    inputCol=\"tld\",\n",
    "    outputCol=\"tld_indexed\", \n",
    "    handleInvalid=\"keep\"  \n",
    ")\n",
    "\n",
    "\n",
    "feature_cols = ['url_length', 'num_dots', 'has_https', 'has_ip', 'num_subdirs', \n",
    "                'num_params', 'suspicious_words', 'tld_indexed', 'special_char_count', \n",
    "                'digits_count', 'entropy']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols, \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "logistic_regression = LogisticRegression(\n",
    "    maxIter=20, \n",
    "    regParam=0.01, \n",
    "    featuresCol='features', \n",
    "    labelCol='label'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[tld_indexer, assembler, logistic_regression])\n",
    "\n",
    "print(\"Pipeline creada con los siguientes stages:\")\n",
    "print(\"1. StringIndexer (TLD -> numérico)\")\n",
    "print(\"2. VectorAssembler (features -> vector)\")\n",
    "print(\"3. LogisticRegression (modelo)\")\n",
    "print(\"\\nEntrenando pipeline...\")\n",
    "\n",
    "pipeline_model = pipeline.fit(train_over)\n",
    "\n",
    "tld_indexer_model = pipeline_model.stages[0]  \n",
    "tld_labels = tld_indexer_model.labels\n",
    "print(\"\\nMapeo de TLD -> Índice numérico (aprendido del conjunto de entrenamiento):\")\n",
    "print(\"=\" * 60)\n",
    "for idx, tld in enumerate(tld_labels[:10]):  # Mostrar los primeros 10\n",
    "    print(f\"{tld:25s} -> {idx}\")\n",
    "if len(tld_labels) > 10:\n",
    "    print(f\"... y {len(tld_labels) - 10} TLDs más\")\n",
    "print(f\"\\nTotal de TLDs únicos aprendidos: {len(tld_labels)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nAplicando pipeline a datos de validación...\")\n",
    "prediction = pipeline_model.transform(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4364aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizConfusion(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce2c205",
   "metadata": {},
   "source": [
    "Vemos unos resultados bastante similares a los anteriores. El recall medio por clases sigue siendo de 0.9980. Los verdaderos negativos se mantienen igual, han aumentado ligeramente los falsos negativos y descendendido los verdaderos positivos. Por tanto, pese a que las diferencias son super pequeñas. Nos quedamos con la opción de undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17d1714",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd760ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_Pipeline(train:DataFrame, regParam: float = 0.1 , maxIter: int = 50) -> PipelineModel:\n",
    "    tld_indexer = StringIndexer(\n",
    "        inputCol=\"tld\",\n",
    "        outputCol=\"tld_indexed\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=feature_cols, \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\",\n",
    "        outputCol=\"scaledFeatures\",\n",
    "        withMean=True,\n",
    "        withStd=True\n",
    "    )\n",
    "\n",
    "    svm = LinearSVC(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"scaledFeatures\",\n",
    "        maxIter=maxIter,\n",
    "        regParam=regParam\n",
    "    )\n",
    "\n",
    "    svm_pipeline = Pipeline(stages=[\n",
    "        tld_indexer,\n",
    "        assembler,\n",
    "        scaler,\n",
    "        svm\n",
    "    ])\n",
    "\n",
    "    svm_model = svm_pipeline.fit(train)\n",
    "\n",
    "    return svm_model\n",
    "\n",
    "svm_model = SVM_Pipeline(train)\n",
    "svm_predictions = svm_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1_score = evaluator.evaluate(svm_predictions)\n",
    "print(f\"\\nF1-Score: {f1_score:.4f}\")\n",
    "\n",
    "print(\"\\nEjemplos de predicciones (primeras 10 filas):\")\n",
    "svm_predictions.select(\"url\", \"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "matrizConfusion(svm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = svm_predictions.groupBy(\"label\", \"prediction\").count()\n",
    "\n",
    "cm_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ac57c",
   "metadata": {},
   "source": [
    "# Arbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arbolDecision_Pipeline(train:DataFrame, maxDepth: int = 8, minInstancesPerNode: int = 20 ,maxBins:int = 32, seed:int = 12418  ) -> PipelineModel:\n",
    "    \"\"\"\n",
    "    Arbol de Decisión con Pipeline para phising_url\n",
    "    \"\"\"\n",
    "    tld_indexer = StringIndexer(\n",
    "        inputCol=\"tld\",\n",
    "        outputCol=\"tld_indexed\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "\n",
    "    # transforma los indices en un vector binario, no sera necesario un VectorIndexer\n",
    "    tld_ohe = OneHotEncoder(\n",
    "        inputCol=\"tld_indexed\",\n",
    "        outputCol=\"tld_ohe\",\n",
    "        dropLast=True\n",
    "    )\n",
    "\n",
    "    feature_cols = ['url_length', 'num_dots', 'has_https', 'has_ip', 'num_subdirs', \n",
    "                    'num_params', 'suspicious_words',\"tld_ohe\", 'special_char_count', \n",
    "                    'digits_count', 'entropy']\n",
    "    \n",
    "    assembler_arbol = VectorAssembler(\n",
    "        inputCols=feature_cols, \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    dt_arbol = DecisionTreeClassifier(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"features\",\n",
    "        maxDepth=maxDepth,\n",
    "        maxBins=maxBins,\n",
    "        minInstancesPerNode=minInstancesPerNode, # para evitar overfitting\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    dt_pipeline = Pipeline(stages=[\n",
    "        tld_indexer,\n",
    "        tld_ohe,\n",
    "        assembler_arbol,\n",
    "        dt_arbol\n",
    "    ])\n",
    "\n",
    "    dt_model = dt_pipeline.fit(train)\n",
    "\n",
    "    return dt_model\n",
    "\n",
    "dt_model = arbolDecision_Pipeline(train)\n",
    "\n",
    "dt_predictions = dt_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "f1_score = evaluator.evaluate(dt_predictions)\n",
    "print(f\"\\nF1-Score: {f1_score:.4f}\")\n",
    "\n",
    "print(\"\\nEjemplos de predicciones (primeras 10 filas):\")\n",
    "dt_predictions.select(\"url\", \"label\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "\n",
    "matrizConfusion(dt_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a32607",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = dt_predictions.groupBy(\"label\", \"prediction\").count()\n",
    "\n",
    "cm_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d17add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8c9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655c4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ba51ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
